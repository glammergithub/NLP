{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPavlov实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一　DeepPavlov数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下载并解压缩数据\n",
    "下载命名实体识别任务（NER）的CoNLL-2003数据，将其放在文件夹data中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##新建data文件夹，使用data.utils中的download_decompress\n",
    "import deeppavlov\n",
    "from deeppavlov.core.data.utils import download_decompress\n",
    "download_decompress('http://lnsigo.mipt.ru/export/deeppavlov_data/conll2003_v2.tar.gz', 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将文本数据解析为机器可读数据集\n",
    "使用包含带有NE（命名实体）标签的推文的语料库.具有NER的典型文件包含有成对标记  \n",
    "不同文档由-DOCSTART为开头  \n",
    "不同的句子由空行分隔  \n",
    "本文仅关注标记,行的第一个元素和最后一个元素,删除它们之间的POS信息  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个NerDatasetReader类,提供读取数据集的功能.  \n",
    "返回一个包含字段train,test,valid的字典,每个字段存储一个样本列表,每个样本由标记和标签组成,例子如下:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'train':[...],'valid':[...],'test':[...]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集中有三个独立的部分  \n",
    "训练模型的训练数据  \n",
    "用于评估和超参数(开始学习过程之前的参数)调整的验证数据  \n",
    "用于最终评估模型的测试数据  \n",
    "这三个部分分别存储在单独的txt文件里"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实现效果**  \n",
    "**将'train.txt','test.txt','valid.txt'及其内容形成字典,字典由元组组成,每一个元组包含了一组关系密切的词对应的tokens和tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DOCSTART>\t0\n",
      "\n",
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n",
      "\n",
      "Peter\tB-PER\n",
      "Blackburn\tI-PER\n",
      "\n",
      "BRUSSELS\tB-LOC\n",
      "1996-08-22\tO\n",
      "\n",
      "The\tO\n",
      "European\tB-ORG\n",
      "Commission\tI-ORG\n",
      "said\tO\n",
      "on\tO\n",
      "Thursday\tO\n",
      "it\tO\n",
      "disagreed\tO\n",
      "with\tO\n",
      "German\tB-MISC\n",
      "advice\tO\n",
      "to\tO\n",
      "consumers\tO\n",
      "to\tO\n",
      "shun\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      "until\tO\n",
      "scientists\tO\n",
      "determine\tO\n",
      "whether\tO\n",
      "mad\tO\n",
      "cow\tO\n",
      "disease\tO\n",
      "can\tO\n",
      "be\tO\n",
      "transmitted\tO\n",
      "to\tO\n",
      "sheep\tO\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "class NerDatasetReader:\n",
    "    def read(self,data_path,provide_pos = False):\n",
    "        self.provide_pos = provide_pos\n",
    "        data_parts = ['train','valid','test']\n",
    "        extension = '.txt'\n",
    "        #print(data_parts)\n",
    "        dataset = {}\n",
    "        for data_part in data_parts:#遍历每一个元素\n",
    "            file_path = Path(data_path) / Path(data_part + extension)#形成文件路径\n",
    "            #print(file_path)\n",
    "            dataset[data_part] = self.read_file(str(file_path))#形成字典,字典的每个部分内容是xxx.txt\n",
    "        return dataset\n",
    "    def read_file(self,file_path):#读文件\n",
    "        samples = []\n",
    "        with open(file_path,'r')as rr:\n",
    "            tokens = ['<DOCSTART>']#tokens标记\n",
    "            pos_tags = ['0']\n",
    "            tags = ['0']#tags标签\n",
    "            for line in rr:\n",
    "                #print(line)\n",
    "                #print(self.povide)\n",
    "                if 'DOCSTART' in line:#第一行\n",
    "                    if len(tokens) > 1:\n",
    "                        if self.provide_pos:\n",
    "                            samples.append(((tokens.pos_tags),tags,))\n",
    "                        else:\n",
    "                            samples.append((tokens,tags))\n",
    "                        tokens = []\n",
    "                        pos_tags = []\n",
    "                        tags = []\n",
    "                elif len(line) < 2:#空行标志着一个段落的结束,一个段落为一句话,有相似成分,遇到空行将tokens和tags列表加到samples列表\n",
    "                    if self.provide_pos:\n",
    "                        samples.append(((tokens,tags),tags))\n",
    "                    else:\n",
    "                        samples.append((tokens,tags))\n",
    "                        tokens = []#清空,进行下一次读取\n",
    "                        pos_tags = []\n",
    "                        tags = []\n",
    "                else:#每一部分的读取\n",
    "                    if self.provide_pos:\n",
    "                        token,*_,pos,tag = line.split()\n",
    "                        pos_tags.append(pos)\n",
    "                    else:#只保留第一个tokens标记和最后一个tags标签\n",
    "                        token,*_,tag = line.split()\n",
    "                    tags.append(tag)\n",
    "                    tokens.append(token)\n",
    "\n",
    "        return samples\n",
    "dataset_reader = NerDatasetReader()#实例化\n",
    "dataset = dataset_reader.read('data/')#找到目录,调用方法\n",
    "#print(dataset['train'])\n",
    "for sample in dataset['train'][:5]:#抽取了前5个结果查看,格式化输出\n",
    "    for token ,tag in zip(*sample):\n",
    "        print('%s\\t%s' %(token,tag))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二　字典\n",
    "训练神经网络使用两个映射  \n",
    "\n",
    "1.{token}->{token id}：token id确定现有的标记在词向量矩阵中行的位置  \n",
    "［参考资料：https://spaces.ac.cn/archives/4122 ]    　　　\n",
    "\n",
    "2.{tag}->{tag id}:tag id用于用于产生独热向量概率分布矢量,计算神经网络输出损失度    \n",
    "(稀疏向量(one-hot)，监督学习正确打标记(ground truth)) \n",
    "\n",
    "下面要实现的目标是实现一个类得到标记和标签与对应id的转化\n",
    "\n",
    "**实现效果**  \n",
    "**实现了标记和标号的映射关系  \n",
    "在之前形成的字典的基础上,将字典里的每一个词的tokens标记和tags标签按照出现次数多少从大到小排序并从前往后标号,去除特殊词,分别存在一个  \n",
    "字典和一个列表里,在之后通过判断输入的是字符串(tokens,tags标记)还是数字(标号),对应返回(标号,标记)  \n",
    "根据标号大小可以判断这个词出现的频率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[[0, 28, 7, 392, 2715], [1260, 0]]\n",
      "[[9, 9, 9], [3, 5]]\n",
      "[['1996-08-26', 'expected', 'party', 'United', 'National', 'now', 'Hong', 'before', '7', 'says']]\n",
      "[['0', 'B-MISC', 'I-ORG', 'I-PER', '0']]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "class NerDatasetReader:\n",
    "    def read(self,data_path,provide_pos = False):\n",
    "        self.provide_pos = provide_pos\n",
    "        data_parts = ['train','valid','test']\n",
    "        extension = '.txt'\n",
    "        dataset = {}\n",
    "        for data_part in data_parts:#遍历每一个元素\n",
    "            file_path = Path(data_path) / Path(data_part + extension)#形成文件路径\n",
    "            dataset[data_part] = self.read_file(str(file_path))#字典\n",
    "        return dataset\n",
    "    def read_file(self,file_path):\n",
    "        samples = []\n",
    "        with open(file_path,'r')as rr:\n",
    "            tokens = ['<DOCSTART>']\n",
    "            pos_tags = ['0']\n",
    "            tags = ['0']\n",
    "            for line in rr:\n",
    "                if 'DOCSTART' in line:\n",
    "                    if len(tokens) > 1:\n",
    "                        if self.provide_pos:\n",
    "                            samples.append(((tokens.pos_tags),tags,))\n",
    "                        else:\n",
    "                            samples.append((tokens,tags))\n",
    "                        tokens = []\n",
    "                        pos_tags = []\n",
    "                        tags = []\n",
    "                elif len(line) < 2:\n",
    "                    if self.provide_pos:\n",
    "                        samples.append(((tokens,tags),tags))\n",
    "                    else:\n",
    "                        samples.append((tokens,tags))\n",
    "                        tokens = []\n",
    "                        pos_tags = []\n",
    "                        tags = []\n",
    "                else:\n",
    "                    if self.provide_pos:\n",
    "                        token,*_,pos,tag = line.split()\n",
    "                        pos_tags.append(pos)\n",
    "                    else:\n",
    "                        token,*_,tag = line.split()\n",
    "                    tags.append(tag)\n",
    "                    tokens.append(token)\n",
    "    \n",
    "        return samples\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self,special_tokens = tuple()):\n",
    "        self.special_tokens = special_tokens\n",
    "        self._t2i = defaultdict(lambda:0)#字典不存在的键键值默认为１,存放标号\n",
    "        self._i2t = []#存放标记/标签\n",
    "    \n",
    "    def fit(self,tokens):\n",
    "        count = 0\n",
    "        self.freqs = Counter(chain(*tokens))#形成一个字典，键是词,值是词出现的次数\n",
    "        for special_token in self.special_tokens:#special_token放在最前面\n",
    "            self._t2i[special_token] = count\n",
    "            self._i2t.append(special_token)\n",
    "            count += 1\n",
    "        for token,freq in self.freqs.most_common():#去除出现在specail_tokens里面的元素，根据出现次数从多到少->从前到后标号\n",
    "            if token not in self._t2i:\n",
    "                self._t2i[token] = count\n",
    "                self._i2t.append(token)\n",
    "                count += 1\n",
    "    \n",
    "    def __call__(self,batch,**kwargs):#通过()方式就可以访问\n",
    "        indices_batch = []\n",
    "        for sample in batch:\n",
    "            indices_batch.append([self[ch] for ch in sample])#self[ch]调用了__getitem__\n",
    "        return indices_batch\n",
    "    \n",
    "    def __getitem__(self,key):#判断是字符串还是数字,返回对应的列表\n",
    "        if isinstance(key,(int,np.integer)):\n",
    "            return self._i2t[key]\n",
    "        elif isinstance(key,str):\n",
    "            return self._t2i[key]\n",
    "        else:\n",
    "            raise NotImplementedError(\"not implemented for type `{}`\".format(type(key)))\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self._i2t)\n",
    "\n",
    "dataset_reader = NerDatasetReader()\n",
    "dataset = dataset_reader.read('data/')\n",
    "\n",
    "special_tokens = ['<UNK>']\n",
    "special_tags = ['O']\n",
    "\n",
    "token_vocab = Vocab(special_tokens)\n",
    "tag_vocab = Vocab(special_tags)\n",
    "\n",
    "all_tokens_by_sentenses = [tokens for tokens, tags in dataset['train']]\n",
    "all_tags_by_sentenses = [tags for tokens, tags in dataset['train']]\n",
    "\n",
    "token_vocab.fit(all_tokens_by_sentenses)\n",
    "tag_vocab.fit(all_tags_by_sentenses)\n",
    "\n",
    "indices_batch = token_vocab([['Yan', 'is', 'a', 'good', 'fellow'],\n",
    " ['For', 'instance']])#生成每个单词对应的ID\n",
    "print(indices_batch)\n",
    "\n",
    "tag_indices_batch = tag_vocab([['0','0','0'],['B-ORG','I-ORG']])#生成每个标签对应的ID\n",
    "print(tag_indices_batch)\n",
    "\n",
    "token_batch = token_vocab([np.random.randint(0,512,size=10)])#10个随机的ID数生成对应ID的token\n",
    "print(token_batch)\n",
    "\n",
    "\n",
    "tag_batch = tag_vocab([np.random.randint(0,10,size = 5)])\n",
    "print(tag_batch)#10个随机数生成对应ID的tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三 数据集迭代器\n",
    "神经网络通常批量训练,意味着神经网络的权重更新每次基于若干个序列.需要遍历数据集然后批量的生成x和y  \n",
    "这里的x类似[['Yan', 'is', 'a', 'good', 'fellow],['For','instance']](token)  \n",
    "  这里的y类似[['B-PER', 'O', 'O', 'O', 'O'],['O', 'O']](tag)  \n",
    "重要的概念在于批处理生成的随机化,随机化就是从数据集中随机地选取一个样本.从大量连续样本的随机数据训练是必要的,有可能得到高质量的模型   \n",
    "一个批次中所有的序列需要有相同的长度 \n",
    "\n",
    "**实现效果**  \n",
    "**能够随机生成一个指定长度的向量,向量的内容为每一组相关联的词组成的列表.因为得到的向量长度可能不同,以最长的向量为准,短的向量不足的部分补0,为之后的CNN神经网络提供数据准备**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 弥补向量缺损长度\n",
    "通过以上产生的数据发现,数据向量的长度不一致,需要生成一个二进制的01向量,1代表存在字符    的位置,0代表不存在的位置,通过填充使得所有序列的长度等于批处理序列中最长的向量长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function iterator at 0x7fa01f53c9d8>\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "def iterator():\n",
    "    data = [1,2,3]\n",
    "    for d in data:\n",
    "        yield d\n",
    "print(iterator)\n",
    "\n",
    "for i in iterator():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((['\"', 'We', 'are', 'united', 'and', 'we', 'are', 'waiting', 'for', 'the', 'government', 'to', 'decide', 'what', 'to', 'do', 'with', 'us', '.', '\"'], ['Manchester', 'City', '3', '1', '0', '2', '2', '3', '3']), (['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from random import Random\n",
    "class NerDatasetReader:\n",
    "    def read(self,data_path,provide_pos = False):\n",
    "        self.provide_pos = provide_pos\n",
    "        data_parts = ['train','valid','test']\n",
    "        extension = '.txt'\n",
    "        dataset = {}\n",
    "        for data_part in data_parts:#遍历每一个元素\n",
    "            file_path = Path(data_path) / Path(data_part + extension)#形成文件路径\n",
    "            dataset[data_part] = self.read_file(str(file_path))#字典\n",
    "        return dataset\n",
    "    def read_file(self,file_path):\n",
    "        samples = []\n",
    "        with open(file_path,'r')as rr:\n",
    "            tokens = ['<DOCSTART>']\n",
    "            pos_tags = ['0']\n",
    "            tags = ['0']\n",
    "            for line in rr:\n",
    "                #print(line)\n",
    "                #print(self.povide)\n",
    "                if 'DOCSTART' in line:\n",
    "                    if len(tokens) > 1:\n",
    "                        if self.provide_pos:\n",
    "                            samples.append(((tokens.pos_tags),tags,))\n",
    "                        else:\n",
    "                            samples.append((tokens,tags))\n",
    "                        tokens = []\n",
    "                        pos_tags = []\n",
    "                        tags = []\n",
    "                elif len(line) < 2:\n",
    "                    if self.provide_pos:\n",
    "                        samples.append(((tokens,tags),tags))\n",
    "                    else:\n",
    "                        samples.append((tokens,tags))\n",
    "                        tokens = []\n",
    "                        pos_tags = []\n",
    "                        tags = []\n",
    "                else:\n",
    "                    if self.provide_pos:\n",
    "                        token,*_,pos,tag = line.split()\n",
    "                        pos_tags.append(pos)\n",
    "                    else:\n",
    "                        token,*_,tag = line.split()\n",
    "                    tags.append(tag)\n",
    "                    tokens.append(token)\n",
    "        \n",
    "        return samples\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self,special_tokens = tuple()):\n",
    "        self.special_tokens = special_tokens\n",
    "        self._t2i = defaultdict(lambda:0)#字典不存在的键键值默认为１\n",
    "        self._i2t = []\n",
    "    \n",
    "    def fit(self,tokens):\n",
    "        count = 0\n",
    "        self.freqs = Counter(chain(*tokens))#形成一个字典，对应每个词出现的次数\n",
    "        #print(self.freqs.items())\n",
    "        for special_token in self.special_tokens:#special_tokens放在最前面\n",
    "            self._t2i[special_token] = count\n",
    "            self._i2t.append(special_token)\n",
    "            count += 1\n",
    "        for token,freq in self.freqs.most_common():#去除出现在specail_tokens里面的元素，根据出现次数从前到后标号\n",
    "            if token not in self._t2i:\n",
    "                self._t2i[token] = count\n",
    "                self._i2t.append(token)\n",
    "                count += 1\n",
    "    \n",
    "    def __call__(self,batch,**kwargs):\n",
    "        indices_batch = []\n",
    "        for sample in batch:\n",
    "            indices_batch.append([self[ch] for ch in sample])\n",
    "        return indices_batch\n",
    "    \n",
    "    def __getitem__(self,key):\n",
    "        if isinstance(key,(int,np.integer)):\n",
    "            return self._i2t[key]\n",
    "        elif isinstance(key,str):\n",
    "            return self._t2i[key]\n",
    "        else:\n",
    "            raise NotImplementedError(\"not implemented for type `{}`\".format(type(key)))\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self._i2t)\n",
    "class DatasetIterator:\n",
    "    def __init__(self,data,seed:int = None):\n",
    "        self.data = {\n",
    "            'train':data['train'],\n",
    "            'valid':data['valid'],\n",
    "            'test':data['test']\n",
    "        }\n",
    "        self.random = Random(seed)\n",
    "    def gen_batches(self,batch_size,data_type = 'train',shuffle = True):\n",
    "        \"\"\"\n",
    "        batch_size为一次选取的批处理的样本个数,这里的一个样本是一个(tokens,tag)元组\n",
    "        data_type为样本类型\n",
    "        shuffle随机化\n",
    "        \"\"\"\n",
    "        data = self.data[data_type]#这里data为数据集里的一组相关联的部分\n",
    "        data_len = len(data)#列表的长度就是元组的数量\n",
    "        if(data_len == 0):\n",
    "            return\n",
    "        \n",
    "        order = list(range(data_len))#0-data_len-1\n",
    "        \n",
    "        if shuffle:\n",
    "            self.random.shuffle(order)#将元组的下标随机化重排\n",
    "        \n",
    "        if batch_size < 0:\n",
    "            batch_size = data_len\n",
    "        \n",
    "        for i in range((data_len - 1) // batch_size + 1):#按照batch_size分块选取批向量\n",
    "            #迭代器将第i个且数量为batch_size个的元组里的tokens,tags分别对应打包在一起\n",
    "            yield tuple(zip(*[data[o] for o in order[i * batch_size:(i + 1) * batch_size]]))\n",
    "class Mask:\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        pass\n",
    "    def __call__(self,tokens_batch,**kwargs):\n",
    "        \"\"\"\n",
    "        接收批量的tokens,返回对应长度的mask列表\n",
    "        \"\"\"\n",
    "        batch_size = len(tokens_batch)\n",
    "        max_len = max(len(utt) for utt in tokens_batch)\n",
    "        mask = np.zeros([batch_size,max_len],dtype = np.float32)#返回一个用0填充的数组\n",
    "        #np.zeros参考资料https://blog.csdn.net/qq_26948675/article/details/54318917\n",
    "        for n,utterance in enumerate(tokens_batch):#返回一个二维数组,第一维代表是第几个列表,第二维表示列表的具体内容\n",
    "            #print(n,utterance)\n",
    "            mask[n,:len(utterance)] = 1\n",
    "        return mask\n",
    "    \n",
    "dataset_reader = NerDatasetReader()\n",
    "dataset = dataset_reader.read('data/')\n",
    "dataset_iterator = DatasetIterator(dataset)\n",
    "next(dataset_iterator.gen_batches(2, shuffle=True))\n",
    "print(next(dataset_iterator.gen_batches(2, shuffle=True)))\n",
    "get_mask = Mask()\n",
    "get_mask([['Try', 'to', 'get', 'the', 'mask'], ['Check', 'paddings']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
