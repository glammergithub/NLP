https://www.cnblogs.com/tq007/p/7281105.html
一、古腾堡语料库
1.挑选文本简·奥斯丁的《爱玛》——并给它一个简短的名称 emma，然后找出它包含多少个词： 
>>> import nltk 
>>> nltk.corpus.gutenberg.fileids() 
>>> emma = nltk.corpus.gutenberg.words('austen-emma.txt')
>>> len(emma)
或者
>>> from nltk.corpus import gutenberg 
>>> gutenberg.fileids() 
>>> emma = gutenberg.words('austen-emma.txt')
>>> len(emma) 
2.找出文本中的索引
>>> emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt')) 
>>>emma.concordance("surprize") 
3.通过循环遍历前面列出的gutenberg文件标识符链表相应的 fileid，然后计算统计每个文本
>>> for fileid in gutenberg.fileids():
...     num_chars = len(gutenberg.raw(fileid))
...     num_words = len(gutenberg.words(fileid))
...     num_sents = len(gutenberg.sents(fileid))
...     num_vocab = len(set([w.lower()for w in gutenberg.words(fileid)]))
...     print(int(num_chars/num_words),int(num_words/num_sents),int(num_words/num_vocab),fileid)
...
这个程序显示每个文本的三个统计量：平均词长、平均句子长度和本文中每个词出现的 平均次数（我们的词汇多样性得分）。平均词长似乎是英语的一个一般属性，因为它的值总是4。（事实上，平均词长是3 而不是4，因为 num_chars变量计数了空白字符。） 相比之下，平均句子长度和词汇多样性看上去是作者个人的特点
4.sents()函数把文本划分成句子，其中每一个句子是一个词链表。
>>> macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt') 
>>> macbeth_sentences#句子链表
>>> macbeth_sentences[1037]#第1038个
>>> longest_len = max([len(s) for s in macbeth_sentences])#求最大的句子长度
>>> [s for s in macbeth_sentences if len(s) == longest_len]
二、网络和聊天文本
虽然古腾堡项目包含成千上万的书籍，它代表既定的文学。考虑较不正式的语言也是 很重要的。NLTK的网络文本小集合的内容包括 Firefox交流论坛，在纽约无意听到的对话，《加勒比海盗》的电影剧本，个人广告和葡萄酒的评论：
>>> from nltk.corpus import webtext
>>> for fileid in webtext.fileids():
...     print(fileid,webtext.raw(fileid)[:65],'...')
...
还有一个即时消息聊天会话语料库，最初由美国海军研究生院为研究自动检测互联网幼童虐待癖而收集的。语料库包含超过10,000张帖子，以“UserNNN”形式的通用名替换掉用户名，手工编辑消除任何其他身份信息，制作而成。语料库被分成 15 个文件，每个文件包含几百个按特定日期和特定年龄的聊天室（青少年、20 岁、30 岁、40 岁、再加上一个通用的成年人聊天室）收集的帖子。文件名中包含日期、聊天室和帖子数量，例如：10-1920s_706posts.xml 包含 2006年10月19日从20多岁聊天室收集的 706个帖子。 
>>> from nltk.corpus import nps_chat
>>> chatroom = nps_chat.posts('10-19-20s_706posts.xml')
>>> chatroom
三、布朗语料库
布朗语料库是第一个百万词级的英语电子语料库的，由布朗大学于 1961 年创建。这个 语料库包含500 个不同来源的文本，按照文体分类，如：新闻、社论等
>>> from nltk.corpus import brown
>>> brown.categories()
>>> brown.words(categories='news')
>>> brown.words(fileids = ['cg22'])
>>> brown.sents(categories = ['new','editorial','reviews'])
布朗语料库是一个研究文体之间的系统性差异——一种叫做文体学的语言学研究——很方便的资源。让我们来比较不同文体中的情态动词的用法。第一步：产生特定文体的计数。记住做下面的实验之前要import nltk
>>> news_text = brown.words(categories = 'news')
>>> fdist = nltk.FreqDist([w.lower()for w in news_text])
>>> modals = ['can','could','may','might','must','will']
>>> for m in modals:
...     print(m + ':',fdist[m])
...
举例
>>> import nltk
>>> from nltk.corpus import brown
>>> cfd = nltk.ConditionalFreqDist(
...     (genre,word)
...     for genre in brown.categories()
...     for word in brown.words(categories=genre)
... )
>>> genres = ['news','religion','hobbies','science_fiction','romance','humor']
>>> modals = ['can','could','may','might','must','will']
>>> cfd.tabulate(conditions = genres,samples = modals)
                  can could   may might  must  will
           news    93    86    66    38    50   389
       religion    82    59    78    12    54    71
        hobbies   268    58   131    22    83   264
science_fiction    16    49     4    12     8    16
        romance    74   193    11    51    45    43
          humor    16    30     8     8     9    13
四、路透社语料库
路透社语料库包含 10,788个新闻文档，共计130万字。这些文档分成90个主题，按照 “训练”和“测试”分为两组。因此，fileid为“test/14826”的文档属于测试组。这样分割是为了训练和测试算法的，这种算法自动检测文档的主题
>>> from nltk.corpus import reuters 
>>> reuters.fileids() 
>>> reuters.categories()
与布朗语料库不同，路透社语料库的类别是有互相重叠的，只是因为新闻报道往往涉及多个主题。我们可以查找由一个或多个文档涵盖的主题，也可以查找包含在一个或多个类别中的文档。为方便起见，语料库方法既接受单个的fileid 也接受fileids列表作为参数
>>> reuters.categories('training/9865')
>>> reuters.categories(['trainging/9865','training/9880'])
>>> reuters.fileids('barley')
>>> reuters.fileids(['barley','corn'])
我们可以以文档或类别为单位查找我们想要的词或句子。这些文本中最开始的 几个词是标题，按照惯例以大写字母存储
>>> reuters.words('training/9865')[:14]
>>> reuters.words(['training/9865','training/9880'])
>>> reuters.words(categories='barley')
>>> reuters.words(categories=['barley','corn'])
五、就职演说语料库
语料库实际上是55个文本的集合，每个文本都是一个总统的演说。这个集合的一个有趣特性是它的时间维度
>>> from nltk.corpus import inaugural
>>> inaugural.fileids()
>>> [fileid[:4] for fileid in inaugural.fileids()]
词汇
america
和
citizen
随时间推移的使用情况。下面的代码使用 w.lower()将就职演说语料库中的词汇转换成小写。然后用 startswith()检查它们是否以“目标”词汇america或citizen开始。因此，它会计算如American's和Citizens等
>>> cfd = nltk.ConditionalFreqDist(
...     (target,fileid[:4])
...     for fileid in inaugural.fileids()
...     for w in inaugural.words(fileid)
...     for target in ['america','citizen']
...     if w.lower().startswith(target)
... )
六、标注文本语料库
许多文本语料库都包含语言学标注，有词性标注、命名实体、句法结构、语义角色等。 NLTK 中提供了很方便的方式来访问这些语料库中的几个
七、其他语言的语料库
NLTK 包含多国语言语料库。某些情况下你在使用这些语料库之前需要学习如何在Pyt hon中处理字符编码
>>> nltk.corpus.cess_esp.words()
>>> nltk.corpus.floresta.words()
>>> nltk.corpus.indian.words('hindi.pos')
>>> nltk.corpus.udhr.fileids()
>>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]
这些语料库的最后，udhr，是超过300种语言的世界人权宣言。这个语料库的fileids包括有关文件所使用的字符编码，如：UTF8或者Latin1。让我们用条件频率分布来研究“世界人权宣言”（udhr）语料库中不同语言版本中的字长差异
>>> from nltk.corpus import udhr
>>> languages = ['Chickasaw', 'English','German_Deutsch',
...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
>>> cfd = nltk.ConditionalFreqDist(
...     (lang,len(word))
...     for lang in languages
...     for word in udhr.words(lang + '-Latin1')
... )
cfd.plot(cumulative = True)
八、文本语料库的结构
通常，文本会按照其可能对应的文体、来源、作者、语言等分类。有时，这些类别会重叠，尤其是在按主题分类的情况下，因为一个文本可能与多个主题相关。
NLTK语料库阅读器支持高效的访问大量语料库，并且能用于处理新的语料库。
NLTK 中定义的基本语料库函数：
fileids() 语料库中的文件 
fileids([categories]) 这些分类对应的语料库中的文件 
categories() 语料库中的分类 
categories([fileids]) 这些文件对应的语料库中的分类 
raw() 语料库的原始内容 
raw(fileids=[f1,f2,f3]) 指定文件的原始内容 
raw(categories=[c1,c2]) 指定分类的原始内容 
words() 整个语料库中的词汇 
words(fileids=[f1,f2,f3]) 指定文件中的词汇 
words(categories=[c1,c2]) 指定分类中的词汇 
sents() 指定分类中的句子 
sents(fileids=[f1,f2,f3]) 指定文件中的句子 
sents(categories=[c1,c2]) 指定分类中的句子 
abspath(fileid) 指定文件在磁盘上的位置 
encoding(fileid) 文件的编码（如果知道的话） 
open(fileid) 打开指定语料库文件的文件流 
root() 到本地安装的语料库根目录的路径
例子
>>> from nltk.corpus import gutenberg
>>> raw = gutenberg.raw("burgess-busterbrown.txt")
>>> raw[1:20]
>>> words = gutenberg.words("burgess-busterbrown.txt")
>>> words[1:20]
>>> sents = gutenberg.sents("burgess-busterbrown.txt")
>>> sents[1:20]
九、载入自己的语料库
1.如果你有自己收集的文本文件，并且想使用前面讨论的方法访问它们，你可以很容易地在NLTK中的 PlaintextCorpusReader帮助下载入它们，PlaintextCorpusReader初始化函数的第二个参数可以是一个['a.txt', 'test/b.txt']这样的fileids链表，或者一个匹配所有fileids的模式，如：'[abc]/.*\.txt'
>>> from nltk.corpus import PlaintextCorpusReader
>>> corpus_root = '/NLP_python'
>>> wordlists = PlaintextCorpusReader(corpus_root,'a.txt')
>>> wordlists.words()
（未成功）
下面是另一个例子，假设你在本地硬盘上有自己的宾州树库（第3版）的拷贝，放在 C:\corpora。我们可以使用BracketParseCorpusReader访问这些语料。我们指定corpus _root为存放语料库中解析过的《华尔街日报》部分的位置，并指定file_pattern 与它的子文件夹中包含的文件匹配（用前斜杠）。 
>>> from nltk.corpus import BracketParseCorpusReader 
>>> corpus_root = r"C:\corpora\penntreebank\parsed\mrg\wsj" 
>>> file_pattern = r".*/wsj_.*\.mrg" 
>>> ptb = BracketParseCorpusReader(corpus_root, file_pattern) 
>>> ptb.fileids() 
['00/wsj_0001.mrg', '00/wsj_0002.mrg','00/wsj_0003.mrg','00/wsj_0004.mrg', ...]
>>> len(ptb.sents()) 
49208
>>> ptb.sents(fileids='20/wsj_2013.mrg')


2.2条件频率分布
当语料文本被分为几类（文体、主题、作者等）时，我们可以计算每个类别独立的频率分布。这将允许我们研究类别之间的系统性差异。在上一节中，我们是用 NLTK 的Condit ionalFreqDist数据类型实现的。条件频率分布是频率分布的集合，每个频率分布有一个不同的“条件”。这个条件通常是文本的类别。
一、条件和事件
频率分布计算观察到的事件，如文本中出现的词汇。条件频率分布需要给每个事件关联一个条件，所以不是处理一个词序列，我们必须处理的是一个配对序列
>>> pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]
每对的形式是：（条件，事件）
二、按文体计数词汇
以布朗语料库的每一部分为条件的条件频率分布，并按照每个条件计数词汇。FreqDist()以一个简单的链表作为输入，ConditionalFreqDist()以一个配对链表作为输入
让我们拆开来看，只看两个文体：新闻和言情。对于每个文体，我们遍历文体中的每个词以产生文体与词的配对
>>> from nltk.corpus import brown
>>> genre_word = [(genre,word) for genre in ['news','romance'] for word in brown.words(categories = genre)]
>>> len(genre_word)
>>> genre_word[:4]
>>> genre_word[-4:]
现在，我们可以使用此配对链表创建一个ConditionalFreqDist，并将它保存在一个变量cfd中。像往常一样，我们可以输入变量的名称来检查它，并确认它有两个条件：
>>> cfd = nltk.ConditionalFreqDist(genre_word)
>>> cfd
>>> cfd.conditions()
>>> cfd['news']
>>> cfd['romance']
>>> list(cfd['romance'])
>>> cfd['romance']['could']
三、绘制分布图和分布表
1.条件是词 america 或 citizen，被绘图的计数是在特定演讲中出现的词的次数。它利用了每个演讲的文件名——例如 1 865-Lincoln.txt——的前4个字符包含年代的事实。这段代码为文件1865-Lincoln.txt 中每个小写形式以america 开头的词——如：Americans——产生一个配对('america', '1865')。 
>>> from nltk.corpus import inaugural
>>> cfd = nltk.ConditionalFreqDist(
...     (target,fileid[:4])
...     for fileid in inaugural.fileids()
...     for w in inaugural.words(fileid)
...     for target in ['america','citizen']
...     if w.lower().startswith(target)
... )
2.条件是语言的名称，图中的计数来源于词长。它利用了每一种语言的文件名是语言名称后面跟'-Latin1' （字符编码）的事实
>>> from nltk.corpus import udhr
>>> languages = ['Chickasaw', 'English', 'German_Deutsch',
...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
>>> cfd = nltk.ConditionalFreqDist(
...     (lang,len(word))
...     for lang in languages
...     for word in udhr.words(lang + '-Latin1')
... )
>>> cfd.tabulate(conditions = ['English','German_Deursch'],samples = range(10),cumulative = True)
                  0    1    2    3    4    5    6    7    8    9
       English    0  185  525  883  997 1166 1283 1440 1558 1638
German_Deursch    0    0    0    0    0    0    0    0    0    0
在 plot()和 tabulate()方法中，我们可以使用 conditions= parameter 来选择指定 哪些条件显示。如果我们忽略它，所有条件都会显示。同样，我们可以使用 samples= p arameter 来限制要显示的样本。这使得载入大量数据到一个条件频率分布，然后通过选定 条件和样品，绘图或制表的探索成为可能。这也使我们能全面控制条件和样本的显示顺序。

四、使用双连词生成随机文本
bigrams()函数接受一个词汇链表，并建立一个连续的词对链表。
>>> sent =  ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',
...     'and','the','earth','.']
>>> list(nltk.bigrams(sent)) 
我们把每个词作为一个条件，对每个词我们有效的创建它的后续词的频率分布
方法一：
函数generate_model()包含一个简单的循环来生成文本。当我们调用这个函数时，我们选择一个词（如“living”）作为我们的初始内容。然后，进入循环后，我们输入变量 word 的当前值，重新设置 word 为上下文中最可能的标识符（使用 max()）。下一次进入循环，我们使用那个词作为新的初始内容。正如你通过检查输出可以看到的，这种简单的文本生成方法往往会在循环中卡住。
>>> def generate_model(cfdist,word,num = 15):
...     for i in range(num):
...             print(word),
...             word = cfdist[word].max()
...
>>> text = nltk.corpus.genesis.words('english-kjv.txt')
>>> bigrams = nltk.bigrams(text)
>>> cfd = nltk.ConditionalFreqDist(bigrams)
>>> print(cfd['living'])
#输出living的词对
<FreqDist with 6 samples and 16 outcomes>
>>> generate_model(cfd,'living')
living
creature
that
he
said
,
and
the
land
of
the
land
of
the
land
living 后最可能出现creature,creature后最可能出现that,以此类推
五、条件频率分布常用方法
cfdist= ConditionalFreqDist(pairs) 从配对链表中创建条件频率分布 
cfdist.conditions() 将条件按字母排序 
cfdist[condition] 此条件下的频率分布 
cfdist[condition][sample] 此条件下给定样本的频率 
cfdist.tabulate() 为条件频率分布制表 
cfdist.tabulate(samples, conditions) 指定样本和条件限制下制表 
cfdist.plot() 为条件频率分布绘图 
cfdist.plot(samples, conditions) 指定样本和条件限制下绘图 
cfdist1 < cfdist2 测试样本在cfdist1中出现次数是否小于在cfdist2中出现次数

2.3更多关于Python：代码重用
代码重用的两个重要方式：文本编辑器和Python函数
一、使用文本编辑器创建程序
通常，使用文本编辑器组织多行程序，然后让 Python一次运行整个程序会更好。
使用IDLE，你可以通过“文件”菜单打开一个新窗口来做到这些
步骤：
1.D:\Anaconda1\Lib\idlelib目录下找到idle.bat文件打开后进入shell
2.新建一个文本编辑器
3.输入print('Monty Python')
4.保存为文件，文件名为monty.py
5.从File中打开该文件，然后运行该文件，在shell里显示
注意：使用所有的小写字母，用下划线分割词汇， 使用.py 文件名后缀，例如：monty_python.py
二、函数
假设你正在分析一些文本，这些文本包含同一个词的不同形式，你的一部分程序需要将给定的单数名词变成复数形式。假设需要在两个地方做这样的事，一个是处理一些文本，另一个是处理用户的输入。 
比起重复相同的代码好几次，把这些事情放在一个函数中会更有效和可靠。一个函数是命名的代码块，执行一些明确的任务，一个函数通常被定义来使用一些称为参数的变量接受一些输入，并且它可能会产生一些结果，也称为返回值。我们使用关键字 def 加函数名以及所有输入参数来定义一个函数，接下来是函数的主体
例子：定义一个简单的函数来处理英文的复数词
def plural(word):
	if word.endswith('y'):
		return word[:-1] + 'ies'
	elif word[-1] in 'sx' or word[-2:] in ['sh','ch']:
		return word + 'es'
	elif word.endswith('an'):
		return word[:-2] + 'en'
	else:
		return word + 's'
>>> plural('fairy')
'fairies'
>>> plural('man')
'men'
三、模块
将自己写的文字处理函数收集到一个单独的地方，访问以前定义的函数不必复制
1.在文本编辑器里复制写好的函数
2.run mudule
3.在shell里
>>> from monty import plural
>>> plural('wish')
'wishes'
>>> plural('fan')
'fen'
在文本编辑器里直接改正这个函数的错误就行
在一个文件中的变量和函数定义的集合被称为一个 Python模块（module）。相关模块的 集合称为一个包（package）。处理布朗语料库的 NLTK代码是一个模块，处理各种不同的语 料库的代码的集合是一个包。NLTK 的本身是包的集合，有时被称为一个库（library）。


